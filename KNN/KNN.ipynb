{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook that will evaluate the KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraires to use\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.neighbors as skl_nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#need to get the processing python file in another directory\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\tree_based\")\n",
    "from process_data import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['snow']\n",
      "New columns: ['temp_fahrenheit']\n",
      "Split: \"train\" \t[Size: 1120] \t[Prec: 0.7]\n",
      "\tX: (1120, 15)\n",
      "\tY: (1120,)\n",
      "Split: \"test\" \t[Size: 480] \t[Prec: 0.3]\n",
      "\tX: (480, 15)\n",
      "\tY: (480,)\n"
     ]
    }
   ],
   "source": [
    "#getting the percentage splits of our training and testing data\n",
    "split_prec = {\n",
    "    'train': 0.7, \n",
    "    'test': 0.3\n",
    "}\n",
    "#whatever scaler we will use\n",
    "scaler = skl_pre.MinMaxScaler()\n",
    "#print(type(scaler))\n",
    "\n",
    "#get the training and testing data\n",
    "X_train, X_test, Y_train, Y_test = process_data(split_prec, scaler)\n",
    "# print(X_train[0])\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  \n",
    "Now that we have our training and test data we will run a KNN model on the training data with k-fold validation for hyperparameter tunning (what is a good value of k, distance measure to use, ect...). Then once we have a decent value of k we will \"retrain\" the model on the entire training data set and use that model on the test data which has never been seen and use the error from that as an estimation of $E_{new}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a trained KNN classifier with the specifications passed\n",
    "def get_fitted_KNN(X,Y, k = 1, met = \"minkowski\"):\n",
    "    classifier = skl_nb.KNeighborsClassifier(n_neighbors=k, metric=met).fit(X,Y)\n",
    "    return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
